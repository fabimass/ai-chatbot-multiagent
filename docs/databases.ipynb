{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases - Research\n",
    "\n",
    "Vector search is a cutting-edge approach to searching and retrieving data that leverages the power of vector similarity calculations. Unlike traditional keyword-based search, which matches documents based on the occurrence of specific terms, vector search focuses on the semantic meaning and similarity of data points. By representing data as vectors in a high-dimensional space, vector search enables more accurate and intuitive search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv==1.0.1 langchain==0.2.1 langchain-community==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load a document. Let's try with a pdf and a markdown file.\n",
    "\n",
    "Then, we will split it into smaller chunks of text. This serves several important purposes:\n",
    "- Granularity: By splitting a document into smaller chunks, you can retrieve more specific and relevant pieces of information. If you work with large documents as single chunks, retrieval can become inefficient and less precise.\n",
    "- Search Performance: Smaller chunks improve the performance of search algorithms. It's easier to match a query against smaller, more focused pieces of text rather than a large document.\n",
    "- Computational efficiency: Working with entire documents as single units can be memory-expensive and slow. Splitting documents into chunks allows for more efficient use of memory and computational resources.\n",
    "\n",
    "Practical example: Imagine we have a large document, such as a product manual or a scientific paper. If a user asks a question like \"How do I reset the device?\" or \"What is the conclusion of the study?\", splitting the document into smaller chunks allows the chatbot to:\n",
    "- Efficiently locate and retrieve the most relevant section about device resetting instructions or the conclusion of the study.\n",
    "- Avoid returning irrelevant parts of the document, which might confuse the user.\n",
    "- Provide a faster response by only processing a small portion of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf==4.2.0 unstructured==0.14.7 markdown==3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define how the text should be split:\n",
    "#  - Each chunk should be up to 512 characters long.\n",
    "#  - There should be an overlap of 64 characters between consecutive chunks. \n",
    "#  - This overlap helps maintain context across the chunks.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 311\n",
      "page_content='GENERATIVE AI: HYPE,  OR \\nTRUL\\nY TRANSFORMATIVE?ISSUE 120 | July 5, 2023 | 12:28 PM EDT”\\x01\\x01\\x01\\x01\\x01P\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01 \\x01\\x01 \\x01 \\x01Global Macro  \\nResearch\\nInvestors should consider this report as only a single factor in making their investment decision. For \\nReg AC certiﬁcation and other important disclosures, see the Disclosure Appendix, or go to www.gs.com/research/hedge.html.\\nThe Goldman Sachs Group, Inc.Since the release of OpenAI’s generative AI tool ChatGPT in November, investor' metadata={'source': 'example_data/example_pdf.pdf', 'page': 0}\n",
      "page_content='interest in generative AI technology has surged. The disruptive potential of  this technology, and whether the hype around it—and market pricing—has gone too far, is Top of Mind. We speak with Conviction’s Sarah Guo, NYU’s Gary Marcus, and GS GIR’s US software and internet analysts Kash Rangan and Eric Sheridan about what the technology can—and can’t—do at this stage. GS economists then assess the technology’s potentially large impact on productivity and growth, which our equity strategists estimate could' metadata={'source': 'example_data/example_pdf.pdf', 'page': 0}\n",
      "page_content='and growth, which our equity strategists estimate could translate into signiﬁcant upside for US equities over the medium-to-longer term, though our strategists also warn that past productivity' metadata={'source': 'example_data/example_pdf.pdf', 'page': 0}\n",
      "page_content='booms have resulted in equity bubbles that ultimately burst. We also discuss where the most compelling investment opportunities in the AI space may lie today, and the near-term risks investors should most watch out for.        \\n““ INTERVIEWS WITH:  \\nSarah Guo, Founder, Conviction, former General Partner, Greylock \\nGary Marcus, Professor Emeritus of Psychology and Neural Science, \\nNew York University \\nKash Rangan, US Software Equity Research Analyst, Goldman Sachs;' metadata={'source': 'example_data/example_pdf.pdf', 'page': 0}\n",
      "page_content='Eric Sheridan, US Internet Equity Research Analyst, Goldman Sachs  \\nAI’S POTENTIALLY LARGE ECONOMIC IMPACTS \\nJoseph Briggs, GS Global Economics Research \\nUS EQUITIES: GAUGING THE AI UPSIDE \\nRyan Hammond and David Kostin, GS US Portfolio Strategy Research \\nMARKETS AROUND PAST PRODUCTIVITY BOOMS \\nDominic Wilson and Vickie Chang, GS Markets Research \\nWHAT WE’RE HEARING FROM PUBLIC INVESTORS \\nPeter Callahan, GS Global Banking & Markets WHAT’S INSIDEof\\nAllison Nathan | allison.nathan@gs.com       ...AND MORE' metadata={'source': 'example_data/example_pdf.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# PDF\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"example_data/example_pdf.pdf\")\n",
    "\n",
    "# Load pdf and split into chunks.\n",
    "pdf_chunks = pdf_loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# Get the number of chunks\n",
    "print(f\"Number of chunks: {len(pdf_chunks)}\")\n",
    "\n",
    "# Print the first 5 chunks\n",
    "for chunk in pdf_chunks[:5]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7\n",
      "page_content='The Football World Cup\\n\\nThe FIFA World Cup, often simply referred to as the World Cup, is the premier international football competition. Organized by the Fédération Internationale de Football Association (FIFA), it features national teams from around the globe competing for the most coveted trophy in the sport. The tournament is held every four years and is one of the most widely viewed and followed sporting events in the world.\\n\\nHistory' metadata={'source': 'example_data/example_markdown.md'}\n",
      "page_content='History\\n\\nThe first World Cup was held in 1930 in Uruguay, with the host nation emerging as the champions. Since then, the tournament has grown significantly in size and popularity. Originally featuring just 13 teams, the World Cup now includes 32 teams in the final tournament, with plans to expand to 48 teams in the near future.\\n\\nFormat\\n\\nThe World Cup is divided into two main stages:' metadata={'source': 'example_data/example_markdown.md'}\n",
      "page_content='Format\\n\\nThe World Cup is divided into two main stages:\\n\\nQualification: This stage occurs over the three years preceding the tournament, with national teams competing within their respective confederations (e.g., UEFA, CONMEBOL, AFC) to secure a spot in the finals.\\n\\nFinal Tournament: Held over approximately one month, the final tournament features:' metadata={'source': 'example_data/example_markdown.md'}\n",
      "page_content='Group Stage: Teams are divided into eight groups of four. Each team plays three matches in a round-robin format. The top two teams from each group advance to the knockout stage.\\n\\nKnockout Stage: A single-elimination format, starting with the Round of 16 and culminating in the final match. This stage includes the Round of 16, Quarter-Finals, Semi-Finals, and the Final.\\n\\nNotable Moments\\n\\nThe World Cup has produced countless memorable moments, including:' metadata={'source': 'example_data/example_markdown.md'}\n",
      "page_content=\"Maracanazo (1950): Uruguay's shocking victory over Brazil in the final match at the Maracanã Stadium.\\n\\nHand of God (1986): Diego Maradona's controversial goal against England in the quarter-finals.\\n\\nGermany 7, Brazil 1 (2014): Germany's stunning semi-final victory over host Brazil.\\n\\nRecords and Statistics\\n\\nMost Titles: Brazil holds the record with five World Cup victories.\\n\\nTop Scorer: Miroslav Klose of Germany is the top scorer in World Cup history with 16 goals.\" metadata={'source': 'example_data/example_markdown.md'}\n"
     ]
    }
   ],
   "source": [
    "# Markdown\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "md_loader = UnstructuredMarkdownLoader(\"example_data/example_markdown.md\")\n",
    "\n",
    "# Load pdf and split into chunks.\n",
    "md_chunks = md_loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# Get the number of chunks\n",
    "print(f\"Number of chunks: {len(md_chunks)}\")\n",
    "\n",
    "# Print the first 5 chunks\n",
    "for chunk in md_chunks[:5]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will proceed to transform our text junks into vector embeddings.\n",
    "\n",
    "Vector embeddings is a powerful technique for transforming complex data into numerical forms that can be easily processed and analyzed by machine learning algorithms. It basically allows us to take virtually any data type and represent it as vectors.\n",
    "\n",
    "But it isn't as simple as just turning data into vectors. We want to ensure that we can perform tasks on this transformed data without losing the data's original meaning. For example, if we want to compare two sentences, we don't want just to compare the words they contain but rather whether or not they mean the same thing. \n",
    "\n",
    "To preserve the data's meaning, we need to understand how to produce vectors where relationships between the vectors make sense. To do this, we need what's known as an embedding model. We apply a pre-trained machine learning model that will produce a representation of this data that is more compact while preserving what's meaningful about the data.\n",
    "\n",
    "The goal of embeddings is to capture the semantic meaning or relationships between data points in a way that similar items are close together in the vector space, and dissimilar items are far apart. For example, consider two words \"king\" and \"queen\". An embedding might map these words to vectors such that the difference between the \"king\" and \"queen\" vectors is similar to the difference between the \"man\" and \"woman\" vectors. This reflects the underlying semantic relationships.\n",
    "\n",
    "Key characteristics of embeddings:\n",
    "- Dimensionality: The number of elements in the vector. Higher dimensions can capture more complex relationships but are computationally more expensive.\n",
    "- Similarity: Measured using metrics like cosine similarity or euclidean distance, which help in finding how close or far two vectors are from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"hello, world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: 768\n",
      "Sample: [0.05168594419956207, -0.030764883384108543, -0.03062233328819275, -0.02802734449505806, 0.01813092641532421]\n"
     ]
    }
   ],
   "source": [
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "google_vector = google_embeddings.embed_query(sample_text)\n",
    "print(f\"Dimensionality: {len(google_vector)}\")\n",
    "print(f\"Sample: {google_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence_transformers==3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: 768\n",
      "Sample: [0.03492263704538345, 0.018829984590411186, -0.017854740843176842, 0.0001388351374771446, 0.0740736797451973]\n"
     ]
    }
   ],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "hf_vector = hf_embeddings.embed_query(sample_text)\n",
    "print(f\"Dimensionality: {len(hf_vector)}\")\n",
    "print(f\"Sample: {hf_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astra DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using Datastax Astra DB:\n",
    "- Create a database in https://astra.datastax.com/\n",
    "- Obtain your database API endpoint, located under Database Details > API Endpoint, and save it as an environment variable called: ASTRA_DB_API_ENDPOINT\n",
    "- Generate a token and save it as an environment variable called: ASTRA_DB_APPLICATION_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-astradb==0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_astradb import AstraDBVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Either an `embedding` or a `collection_vector_service_options`                    must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vstore \u001b[38;5;241m=\u001b[39m \u001b[43mAstraDBVectorStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastra_vector_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mASTRA_DB_API_ENDPOINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mASTRA_DB_APPLICATION_TOKEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masso\\OneDrive\\Escritorio\\rag-chatbot\\.venv\\lib\\site-packages\\langchain_astradb\\vectorstores.py:256\u001b[0m, in \u001b[0;36mAstraDBVectorStore.__init__\u001b[1;34m(self, collection_name, embedding, token, api_endpoint, astra_db_client, async_astra_db_client, namespace, metric, batch_size, bulk_insert_batch_concurrency, bulk_insert_overwrite_concurrency, bulk_delete_concurrency, setup_mode, pre_delete_collection, metadata_indexing_include, metadata_indexing_exclude, collection_indexing_policy, collection_vector_service_options, collection_embedding_api_key)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Embedding and the server-side embeddings are mutually exclusive,\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# as both specify how to produce embeddings\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m collection_vector_service_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither an `embedding` or a `collection_vector_service_options`\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124m            must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m collection_vector_service_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one of `embedding` or `collection_vector_service_options`\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124m            can be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Either an `embedding` or a `collection_vector_service_options`                    must be provided."
     ]
    }
   ],
   "source": [
    "vstore = AstraDBVectorStore(\n",
    "    embedding=embe,\n",
    "    collection_name=\"astra_vector_db\",\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
