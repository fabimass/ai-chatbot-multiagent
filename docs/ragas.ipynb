{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas\n",
    "\n",
    "Ragas is a library that provides tools to supercharge the evaluation of Large Language Model (LLM) applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import SingleTurnSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "        deployment_name=\"gpt-4o\",\n",
    "        api_version=\"2023-06-01-preview\"\n",
    "    )\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"ada-002\", openai_api_version=\"2024-06-01\")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"rag\": [\n",
    "        SingleTurnSample(\n",
    "            user_input=\"cual es el objetivo del proyecto de Fabian?\",\n",
    "            response=\"El objetivo del proyecto de Fabián es desarrollar un chatbot especializado que pueda ser entrenado con documentos propietarios de una empresa y que, en base al contenido ingestado, interprete correctamente consultas de usuarios y proporcione respuestas precisas y relevantes.\",\n",
    "            retrieved_contexts=[\n",
    "                'Plan de proyecto del Trabajo Final\\nCarrera de Especializaci´ on en Inteligencia Artificial\\nIng. Fabi´ an Alejandro Massotto\\n1. Descripci´ on t´ ecnica-conceptual del proyecto a realizar\\nEl objetivo de este proyecto es desarrollar un chatbot especializado que pueda ser entrenado\\ncon documentos propietarios de una empresa, y que, en base al contenido ingestado, interprete\\ncorrectamente consultas de usuarios y proporcione respuestas precisas y relevantes.', \n",
    "                'Plan de proyecto del Trabajo Final\\nCarrera de Especializaci´ on en Inteligencia Artificial\\nIng. Fabi´ an Alejandro Massotto\\nActa de constituci´ on del proyecto\\nBuenos Aires, 23 de abril de 2024\\nPor medio de la presente se acuerda con el Ing. Fabi´ an Alejandro Massotto que su Trabajo\\nFinal de la Carrera de Especializaci´ on en Inteligencia Artificial se titular´ a “Desarrollo de un\\nchatbot especializado para optimizar la b´ usqueda de informaci´ on en documentos propietarios” y',\n",
    "                'Plan de proyecto del Trabajo Final\\nCarrera de Especializaci´ on en Inteligencia Artificial\\nIng. Fabi´ an Alejandro Massotto\\nLa propuesta de valor de este proyecto radica en su capacidad para mejorar la eficiencia operativa\\nde una empresa. Al facilitar el acceso a la informaci´ on, se reduce el tiempo dedicado a la\\nb´ usqueda y se incrementa el tiempo disponible para tareas cr´ ıticas y estrat´ egicas. Adem´ as,\\nla capacidad de adecuar el chatbot seg´ un las necesidades y documentos de cada empresa lo'\n",
    "            ], \n",
    "        )\n",
    "    ],\n",
    "    \"sql\": [],\n",
    "    \"csv\": [],\n",
    "    \"api\": [\n",
    "        SingleTurnSample(\n",
    "            user_input=\"cuantos repositorios tiene el usuario fabimass?\",\n",
    "            response=\"El usuario fabimass tiene 25 repositorios.\",\n",
    "            retrieved_contexts=[\n",
    "                \"\"\"```python\n",
    "        import requests\n",
    "\n",
    "        username = 'fabimass'\n",
    "        url = f'https://api.github.com/users/{username}/repos'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            repos = response.json()\n",
    "            result = len(repos)\n",
    "        else:\n",
    "            result = f'Error: {response.status_code}'\n",
    "\n",
    "        result\n",
    "        ```\"\"\", \n",
    "                '25',\n",
    "            ], \n",
    "        )\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision)\n",
    "\n",
    "Context Precision is a metric that measures the proportion of relevant chunks in the `retrieved_contexts`. It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.\n",
    "\n",
    "$$\n",
    "\\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Precision@k} = {\\text{true positives@k} \\over  (\\text{true positives@k} + \\text{false positives@k})}\n",
    "$$\n",
    "\n",
    "Where $K$ is the total number of chunks in `retrieved_contexts` and $v_k \\in \\{0, 1\\}$ is the relevance indicator at rank $k$.\n",
    "\n",
    "The following metrics uses LLM to identify if a retrieved context is relevant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent:\n",
      "0.99999999995\n"
     ]
    }
   ],
   "source": [
    "print(\"RAG agent:\")\n",
    "for sample in samples[\"rag\"]:\n",
    "    print(await context_precision.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API agent:\n",
      "0.99999999995\n"
     ]
    }
   ],
   "source": [
    "print(\"API agent:\")\n",
    "for sample in samples[\"api\"]:\n",
    "    print(await context_precision.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance)\n",
    "\n",
    "The `ResponseRelevancy` metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.  \n",
    "\n",
    "This metric is calculated using the `user_input` and the `response` as follows:  \n",
    "\n",
    "1. Generate a set of artificial questions (default is 3) based on the response. These questions are designed to reflect the content of the response.  \n",
    "2. Compute the cosine similarity between the embedding of the user input ($E_o$) and the embedding of each generated question ($E_{g_i}$).  \n",
    "3. Take the average of these cosine similarity scores to get the **Answer Relevancy**:  \n",
    "\n",
    "$$\n",
    "\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{cosine similarity}(E_{g_i}, E_o)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{E_{g_i} \\cdot E_o}{\\|E_{g_i}\\| \\|E_o\\|}\n",
    "$$  \n",
    "\n",
    "Where:  \n",
    "- $E_{g_i}$: Embedding of the $i^{th}$ generated question.  \n",
    "- $E_o$: Embedding of the user input.  \n",
    "- $N$: Number of generated questions (default is 3).  \n",
    "\n",
    "**Note**: While the score usually falls between 0 and 1, it is not guaranteed due to cosine similarity's mathematical range of -1 to 1.\n",
    "\n",
    "An answer is considered relevant if it directly and appropriately addresses the original question. This metric focuses on how well the answer matches the intent of the question, without evaluating factual accuracy. It penalizes answers that are incomplete or include unnecessary details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent:\n",
      "0.9678979668947822\n"
     ]
    }
   ],
   "source": [
    "print(\"RAG agent:\")\n",
    "for sample in samples[\"rag\"]:\n",
    "    print(await scorer.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API agent:\n",
      "0.9826498514154302\n"
     ]
    }
   ],
   "source": [
    "print(\"API agent:\")\n",
    "for sample in samples[\"api\"]:\n",
    "    print(await scorer.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import  SingleTurnSample, MultiTurnSample, EvaluationDataset\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import TopicAdherenceScore\n",
    "\n",
    "\n",
    "sample_input = [\n",
    "HumanMessage(content=\"cuantos repositorios tiene el usuario fabimass?\"),\n",
    "AIMessage(content=\"El usuario fabimass tiene 25 repositorios\"),\n",
    "HumanMessage(content=\"cual es la capital de Francia?\"),\n",
    "AIMessage(content=\"No lo se\"),\n",
    "]\n",
    "\n",
    "\n",
    "sample = MultiTurnSample(user_input=sample_input, reference_topics=[\"github\"])\n",
    "scorer = TopicAdherenceScore(llm = evaluator_llm, mode=\"precision\")\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import ToolCallAccuracy\n",
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "\n",
    "sample = [\n",
    "    HumanMessage(content=\"What's the weather like in New York right now?\"),\n",
    "    AIMessage(content=\"The current temperature in New York is 75°F and it's partly cloudy.\", tool_calls=[\n",
    "        ToolCall(name=\"weather_check\", args={\"location\": \"New York\"})\n",
    "    ]),\n",
    "    HumanMessage(content=\"Can you translate that to Celsius?\"),\n",
    "    AIMessage(content=\"Let me convert that to Celsius for you.\", tool_calls=[\n",
    "        ToolCall(name=\"temperature_conversion\", args={\"temperature_fahrenheit\": 75})\n",
    "    ]),\n",
    "    ToolMessage(content=\"75°F is approximately 23.9°C.\"),\n",
    "    AIMessage(content=\"75°F is approximately 23.9°C.\")\n",
    "]\n",
    "\n",
    "sample = MultiTurnSample(\n",
    "    user_input=sample,\n",
    "    reference_tool_calls=[\n",
    "        ToolCall(name=\"fabi\", args={\"location\": \"New York\"}),\n",
    "        ToolCall(name=\"temperature_conversion\", args={\"temperature_fahrenheit\": 75})\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer = ToolCallAccuracy()\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
